{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9035cdbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0582c7",
   "metadata": {},
   "source": [
    "# Práctica de selección de características\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "El objetivo de esta práctica es trabajar con algunas de las técnicas de selección de características estudiadas en  las sesiones teóricas utilizando scikit learn. Debes aplicar los conocimientos adquiridos en las clases de teoría a la resolución de los casos que se proponen y analizar los resultados.\n",
    "\n",
    "## Enunciado\n",
    "En esta práctica trabajaremos la selección de características mediante filtros. Para ello, se presenta un ejemplo completo de selección de características en el caso de predictores categóricos y respuesta categórica. El ejemplo incluye:\n",
    "\n",
    "* Lectura de un dataset\n",
    "* Codificación del dataset\n",
    "* Selección de características usando $\\chi^{2}$\n",
    "* Selección de características usando *mutual information*\n",
    "* Comparación de ambas selecciones usando un modelo de test\n",
    "\n",
    "La tarea que has de realizar consiste en completar los dos casos propuestos en este block de notas de manera análoga al ejemplo presentado.\n",
    "\n",
    "Para ello será necesario estudiar y comprender el caso resuelto y usar la documentación de [scikitlearn](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) para resolver los dos casos pendientes:\n",
    "* Predictores numéricos, respuesta categórica\n",
    "* Predictores numéricos, respuesta numérica\n",
    "\n",
    "El entregable de esta tarea consistirá en un block de notas, con los siguientes requisitos, completado por el grupo:\n",
    "\n",
    "* **Código** para resolver los casos propuestos\n",
    "* **Explicación del código**: comenta tu código de forma que se entienda cómo funciona.\n",
    "* **Análisis e interpretación de los resultados**: analiza y comenta los resultados y relaciónalos con la teoría. Esto incluye el análisis e interpretación de los resultados del caso resuelto. Emplea gráficas para ilustrar el análisis cuando lo consideres oportuno.\n",
    "\n",
    "Has de entregar un fichero comprimido con **ZIP** (y sólo ZIP) que contenga lo siguiente:\n",
    "* **Block de notas** con el código, los análisis y los comentarios de *todos* los casos (incluido el que está resuelto)\n",
    "* **Directorio con los datos**. Los nombres de ficheros que uses en el block de notas **han de ser relativos** a ese directorio (han de empezar por `'./nombre_directorio'`; ver lectura de fichero del caso resuelto). La idea es que al descomprimir el fichero ZIP se pueda ejecutar el block de notas sin tener que cambiar ni una coma.\n",
    "* **Este trabajo se hace en grupo**, así que basta con que un integrante del grupo haga la entrega."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa88c96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c94cf4",
   "metadata": {
    "heading_collapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Caso resuelto: predictores categóricos, respuesta categórica\n",
    "\n",
    "dataset: `breast-cancer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de6100",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Lectura y codificación del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c620c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# example of mutual information feature selection for categorical data\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    data = read_csv(filename, header=None)\n",
    "    # retrieve numpy array\n",
    "    dataset = data.values\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1]\n",
    "    # format all fields as string\n",
    "    X = X.astype(str)\n",
    "    return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    oe = OrdinalEncoder()\n",
    "    oe.fit(X_train)\n",
    "    X_train_enc = oe.transform(X_train)\n",
    "    X_test_enc = oe.transform(X_test)\n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset('./datos/'+'breast-cancer.csv')\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc41f07",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "column_names = ['age', 'menopause', 'tumor-size', 'inv-nodes', \n",
    "                'node-caps','deg-malig', 'breast', 'breast-quad','Class']\n",
    "pd.DataFrame(X_train, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83c28d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_enc, columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb34dd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b014c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# feature selection mutual information\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "# what are scores for the features\n",
    "print(sorted(fs.scores_))\n",
    "for i in range(len(fs.scores_)):\n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "# plot the scores\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b797f92",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### $\\chi^{2}$     (chi cuadrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5b87b",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# feature selection\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    fs = SelectKBest(score_func=chi2, k=4)\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\n",
    "print(X_train_fs.shape)\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)):\n",
    "    print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "# plot the scores\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d851bb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "¿Cómo saber qué características se han seleccionado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c89e4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(fs.get_feature_names_out())\n",
    "print(fs.get_support())\n",
    "print(np.nonzero(fs.get_support()))\n",
    "print([column_names[idx] for (idx, item) in enumerate(fs.get_support()) if item])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481187a2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Comparación de rendimiento\n",
    "\n",
    "Evaluación de las dos selecciones de características usando como referencia un modelo de regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37acb3c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def mi_mutual_info_classif(*args):\n",
    "    score = mutual_info_classif(args[0], args[1], \n",
    "                                discrete_features=True)\n",
    "    return score\n",
    "\n",
    "def select_features(X_train, y_train, X_test, func, k=4):\n",
    "    fs = SelectKBest(score_func=func, k=k)\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "\n",
    "l = []\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "for k in range(1, 10):\n",
    "#     print(f'k={k}')\n",
    "\n",
    "    # feature selection\n",
    "    X_train_fs, X_test_fs, fs = select_features(X_train_enc, \n",
    "                                                y_train_enc, \n",
    "                                                X_test_enc, chi2, k)\n",
    "    # fit the model using MI\n",
    "    model.fit(X_train_fs, y_train_enc)\n",
    "    # evaluate the model\n",
    "    yhat = model.predict(X_test_fs)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test_enc, yhat)\n",
    "#     print('Accuracy chi2: %.2f' % (accuracy*100))\n",
    "\n",
    "    # feature selection\n",
    "    X_train_fs, X_test_fs, fs = select_features(X_train_enc, \n",
    "                                                y_train_enc, \n",
    "                                                X_test_enc,\n",
    "#                                                 mutual_info_classif,\n",
    "                                                mi_mutual_info_classif,\n",
    "                                                k)\n",
    "\n",
    "    # fit the model using mutual information\n",
    "    model.fit(X_train_fs, y_train_enc)\n",
    "    # evaluate the model\n",
    "    yhat = model.predict(X_test_fs)\n",
    "    # evaluate predictions\n",
    "    accuracy_mi = accuracy_score(y_test_enc, yhat)\n",
    "#     print('Accuracy mut. inf.: %.2f' % (accuracy_mi*100))\n",
    "#     print('')\n",
    "    l += [(k, accuracy, accuracy_mi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54258bb6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'chi2': [k for _, k, _ in l],\n",
    "                   'mi': [k for _, _, k in l]}, index=range(1,10))\n",
    "from IPython.display import display\n",
    "display(df)\n",
    "\n",
    "df.plot(style='o-', grid=True, figsize=(9,6),\n",
    "        xticks=[i for i in range(1,10)],\n",
    "        title='Precisión vs. número de características', \n",
    "        xlabel='Número de características',\n",
    "        ylabel='accuracy score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e1749",
   "metadata": {
    "heading_collapsed": true,
    "toc-hr-collapsed": true
   },
   "source": [
    "## Caso propuesto 1: predictores numéricos, respuesta categórica\n",
    "\n",
    "dataset: `pima-indians-diabetes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f16b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif, chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee151ba",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "### Lectura del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\n",
    "    'npreg',\n",
    "    'glucose',\n",
    "    'diastolic',\n",
    "    'triceps',\n",
    "    'insulin',\n",
    "    'bmi',\n",
    "    'dpf',\n",
    "    'age',\n",
    "    'class'\n",
    "]\n",
    "\n",
    "df = pd.read_csv('./datos/pima-indians-diabetes.csv', names=headers)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d09431",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('class', axis=1), df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97442893",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Estadístico F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ec7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_fstat = SelectKBest(score_func=f_classif, k=4).fit(X_train, y_train)\n",
    "X_train_fs = fs_fstat.transform(X_train)\n",
    "plt.bar([i for i in range(len(fs_fstat.scores_))], fs_fstat.scores_)\n",
    "plt.bar_label(plt.gca().containers[0], fmt='%1.2f')\n",
    "plt.xticks([i for i in range(len(fs_fstat.scores_))], X_train.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc762be2",
   "metadata": {},
   "source": [
    "Algo que se puede realizar es ordenar las características por el estadístico.\n",
    "Entonces, aplicando una differenciación numérica se puede ver en cuales características se produce un cambio brusco en el estadístico.\n",
    "Con la idea de que se puede ver en cual característica hay un  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93538495",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(fs_fstat.scores_)[::-1]\n",
    "diff = np.diff(fs_fstat.scores_[order])\n",
    "plt.plot([i for i in range(len(order))], fs_fstat.scores_[order],\"o-\")\n",
    "plt.plot([1+i for i in range(len(diff))], -diff, \"o-\", color='red')\n",
    "plt.xticks([i for i in range(len(order))], X_train.columns[order])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6a380e",
   "metadata": {},
   "source": [
    "En este caso, se puede ver de que la segunda característica ya tiene una gran caída en el estadístico, y sigue bajando desde allí, por lo que no hay ningun numero trivial de características que se puedan quitar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aeef94",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Mutual information (caso clasificación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_mi = SelectKBest(score_func=mutual_info_classif, k=4).fit(X_train, y_train)\n",
    "plt.bar([i for i in range(len(fs_mi.scores_))], fs_mi.scores_)\n",
    "plt.bar_label(plt.gca().containers[0], fmt='%1.2f')\n",
    "plt.xticks([i for i in range(len(fs_mi.scores_))], X_train.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(fs_mi.scores_)[::-1]\n",
    "diff = np.diff(fs_mi.scores_[order])\n",
    "plt.plot([i for i in range(len(order))], fs_mi.scores_[order],\"o-\")\n",
    "plt.plot([1+i for i in range(len(diff))], -diff, \"o-\", color='red')\n",
    "plt.xticks([i for i in range(len(order))], X_train.columns[order])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd27ae5",
   "metadata": {},
   "source": [
    "En este caso, mientras que también hay una gran diferencia en solo la segunda característica, existe otro máximo local en la séptima característica, por lo que se podría razonar que esto es un buen corte en cuanto al número de características."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef727d2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Modelo con todas las características\n",
    "\n",
    "Usar el modelo `LogisticRegression(solver='liblinear')` y la métrica `accuracy_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce28b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "accuracy_score(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32623125",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Modelo con las 4 mejores características descubiertas con F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca06c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fsf = fs_fstat.transform(X_train)\n",
    "X_test_fsf  = fs_fstat.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear').fit(X_train_fsf, y_train)\n",
    "yhat = model.predict(X_test_fsf)\n",
    "accuracy_score(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14f0aa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Modelo con las 4 mejores características descubiertas con *mutual information*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fsmi = fs_mi.transform(X_train)\n",
    "X_test_fsmi  = fs_mi.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear').fit(X_train_fsmi, y_train)\n",
    "yhat = model.predict(X_test_fsmi)\n",
    "accuracy_score(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac92485e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Comparativa de resultados  y comentarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9472283",
   "metadata": {},
   "source": [
    "En estos casos parece ser que la selección de características puede tener algunos positivos.\n",
    "Mientras que no mejore la precision, la bajada es muy baja, de un 0.753 a un 0.744 y 0.740 respectivamente,\n",
    "que no es una bajada muy significativa.\n",
    "Mientras que por el otro lado la reducción de características puede tener positivos externos\n",
    "como una mejor explicabilidad del modelo, una reducción de la complejidad del modelo, una subida en su explicabilidad, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dbc7ad",
   "metadata": {},
   "source": [
    "En cuanto a la comparativa de los dos métodos, son congruentes en 3 de las 4 características que se han seleccionado, con un desacuerdo entre si se debe seleccionar el número de embarrzos o ls niveles de insulina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e4d8f7",
   "metadata": {
    "heading_collapsed": true,
    "toc-hr-collapsed": true
   },
   "source": [
    "## Caso propuesto 2: predictores numéricos, respuesta numérica\n",
    "\n",
    "En este caso usaremos un dataset sintético, generado con la función `make_regression()` de scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6748cc0",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Generación del dataset sintético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd19e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=10,\n",
    "                       noise=0.1, random_state=1)\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,\n",
    "                                                    random_state=1)\n",
    "# summarize\n",
    "print('Train', X_train.shape, y_train.shape)\n",
    "print('Test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596b7ef",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Regresion (`f_regression()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c3dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# feature selection\n",
    "fs = SelectKBest(score_func=f_regression, k=10).fit(X_train, y_train)\n",
    "plt.bar(range(100), fs.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_f = np.argsort(fs.scores_)[::-1]\n",
    "diff_f = np.diff(fs.scores_[order_f])\n",
    "plt.plot(range(15), fs.scores_[order_f][:15],\"o-\")\n",
    "plt.plot(range(15), -diff_f[:15], \"o-\", color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca30b3",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Mutual information (caso regresión)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "fs = SelectKBest(score_func=mutual_info_regression, k=10).fit(X_train, y_train)\n",
    "plt.bar(range(100), fs.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92021167",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_mi = np.argsort(fs.scores_)[::-1]\n",
    "diff_mi = np.diff(fs.scores_[order_mi])\n",
    "plt.plot(range(15), fs.scores_[order_mi][:15],\"o-\")\n",
    "plt.plot(range(15), -diff_mi[:15], \"o-\", color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c8dbb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Modelo con todas las características\n",
    "\n",
    "Usar el modelo `LinearRegression()` y la métrica `mean_absolute_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa590c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# define model\n",
    "lr = LinearRegression()\n",
    "# fit model\n",
    "lr.fit(X_train, y_train)\n",
    "# evaluate model\n",
    "yhat = lr.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print(f'MAE: {mae:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b6bf27",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Modelo con las 10 mejores características obtenidas con la regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75967381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "fs = SelectKBest(score_func=f_regression, k=10).fit(X_train, y_train)\n",
    "X_train_fs = fs.transform(X_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_fs, y_train)\n",
    "yhat = lr.predict(X_test_fs)\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "\n",
    "print(f'MAE: {mae:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb476800",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Modelo con las 10 mejores características obtenidas con *mutual information*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "fs = SelectKBest(score_func=mutual_info_regression, k=10).fit(X_train, y_train)\n",
    "X_train_fs = fs.transform(X_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_fs, y_train)\n",
    "yhat = lr.predict(X_test_fs)\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "\n",
    "print(f'MAE: {mae:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2ab19",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Comparativa de resultados y comentarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c623307c",
   "metadata": {},
   "source": [
    "En este caso, ambos intentos de filtración tienen efectos nocivos, subiendo el error por órdenes de magnitud.\n",
    "Al comprobar los estadísticos ordenados, se pueden ver que hay un par de características que tienen un valor muy alto, y entonces unas 4 o 5 más con un valor medio, hasta que el resto de características tienen un valor muy bajo indistinguible entre ellas.\n",
    "De forma que si 10 de ellas son informativas, al intentar filtrar por las 10 mejores,\n",
    "habran unas cuantas que no podrá capturar correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cedafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(order_mi[:10]) & set(order_f[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b23dd9",
   "metadata": {},
   "source": [
    "En cuanto a la comparativa entre los dos, solo concuerdan en 5 variables.\n",
    "Lo que alude a que solo son capaces en capturar correctamente la importancia de estas 5 variables.\n",
    "Teniendo en cuenta que 10 de ellas se suponen que son informativas, esto es un resultado bastante pobre."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "144.5px",
    "width": "383.234375px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "124093b7be10c32076c75f60f415d6def65edeb693f6c465ae4e1e508e9d8137"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
