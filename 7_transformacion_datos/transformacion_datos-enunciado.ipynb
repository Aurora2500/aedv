{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecedbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "datadir = './datos/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95c17b4",
   "metadata": {},
   "source": [
    "# Ejercicios de transformación de datos\n",
    "\n",
    "En las siguientes secciones del block de notas se plantean ejercicios relacionados con las transformaciones de datos vistas en clase hasta ahora. Cada sección contiene un «esqueleto» de código que debe ser completado para solucionar las cuestiones planteadas en los respectivos encabezados, junto con un enlace a la documentación correspondiente de [scikit-learn](https://scikit-learn.org).\n",
    "\n",
    "En general se trata de preparar los datos, entrenar un modelo y observar y comentar los resultados obtenidos, haciendo en algunos casos, algunas comparativas.\n",
    "\n",
    "**Importante**: recuerda que para evitar filtración de datos (data leaking) hay que calcular los parámetros de las transformaciones (fit) usando únicamente el conjunto de entrenamiento; posteriormente se aplican (transform) tanto al conjunto de entrenamiento como al de test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51fcd4",
   "metadata": {},
   "source": [
    "## Codificación categórico $\\rightarrow$ numérico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aec205",
   "metadata": {},
   "source": [
    "### One hot encoder\n",
    "\n",
    "[Documentación en scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=onehotencoder#sklearn.preprocessing.OneHotEncoder)\n",
    "\n",
    "* dataset: *breast-cancer.csv*\n",
    "* modelo: regresión logística\n",
    "* medida de rendimiento: `accuracy_score`\n",
    "* `OneHotEncoder`\n",
    "* `LabelEncoder`\n",
    "\n",
    "Prueba a usar codificación 1 a k y codificación 1 a k-1 (siendo k el número de categorías) y comenta los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2576d9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# evaluate logistic regression on the breast cancer dataset with a one-hot encoding\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "# load the dataset\n",
    "dataset = read_csv(datadir + 'breast-cancer.csv', header=None)\n",
    "# retrieve the array of data\n",
    "data = dataset.values\n",
    "# separate into input and output columns\n",
    "X = data[:, :-1].astype(str)\n",
    "y = data[:, -1].astype(str)\n",
    "# split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# one-hot encode input variables\n",
    "ohe_full = OneHotEncoder()\n",
    "ohe_reduced = OneHotEncoder(drop=\"first\")\n",
    "\n",
    "X_train_encoded_full = ohe_full.fit_transform(X_train)\n",
    "X_test_encoded_full = ohe_full.transform(X_test)\n",
    "\n",
    "X_train_encoded_reduced = ohe_reduced.fit_transform(X_train)\n",
    "X_test_encoded_reduced = ohe_reduced.transform(X_test)\n",
    "\n",
    "# label encode target variable\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# define the model\n",
    "\n",
    "lr_full = LogisticRegression()\n",
    "lr_reduced = LogisticRegression()\n",
    "\n",
    "# fit on the training set\n",
    "\n",
    "lr_full.fit(X_train_encoded_full, y_train_encoded)\n",
    "lr_reduced.fit(X_train_encoded_reduced, y_train_encoded)\n",
    "\n",
    "# predict on test set\n",
    "\n",
    "yhat_full = lr_full.predict(X_test_encoded_full)\n",
    "yhat_reduced = lr_reduced.predict(X_test_encoded_reduced)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy_full = accuracy_score(yhat_full, y_test_encoded)\n",
    "accuracy_reduced = accuracy_score(yhat_reduced, y_test_encoded)\n",
    "print(f\"Accuracy full rank: {accuracy_full:.6%}\\tAccuracy reduced: {accuracy_reduced:.6%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35a03301",
   "metadata": {},
   "source": [
    "Se puede observar que ambos métodos de codificación, sea 1 a k ó 1 a k-1, ambos dan resultados idénticos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c8cfe",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Ordinal Encoder\n",
    "\n",
    "[Documentación en scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)\n",
    "\n",
    "* dataset: *breast-cancer.csv*\n",
    "* modelo: regresión logística\n",
    "* medida de rendimiento: `accuracy_score`\n",
    "* `OrdinalEncoder`\n",
    "* `LabelEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e2943",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# evaluate logistic regression on the breast cancer dataset with an ordinal encoding\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "# load the dataset\n",
    "dataset = read_csv(datadir + 'breast-cancer.csv', header=None)\n",
    "# retrieve the array of data\n",
    "data = dataset.values\n",
    "# separate into input and output columns\n",
    "X = data[:, :-1].astype(str)\n",
    "y = data[:, -1].astype(str)\n",
    "# split the dataset into train and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# ordinal encode input variables\n",
    "\n",
    "oe = OrdinalEncoder()\n",
    "\n",
    "X_train_encoded = oe.fit_transform(X_train)\n",
    "X_test_encoded = oe.transform(X_test)\n",
    "\n",
    "# label encode target variable\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# define the model\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# fit on the training set\n",
    "\n",
    "lr.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# predict on test set\n",
    "\n",
    "yhat = lr.predict(X_test_encoded)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(yhat, y_test_encoded)\n",
    "print(f'Accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7630e2f9",
   "metadata": {},
   "source": [
    "## Escalado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0904c870",
   "metadata": {},
   "source": [
    "Para temas de simplificación de código mediante generalización, se creará un \"escalar\" de identidad, que no hará nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityScaler:\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359cf6b2",
   "metadata": {},
   "source": [
    "### Escalado [0, 1]\n",
    "\n",
    "[Documentación en scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range)\n",
    "\n",
    "* dataset: *pima-indians-diabetes.csv*\n",
    "* modelo: KNN (pruebas con diferente números de vecinos; ver código)\n",
    "* medida de rendimiento: `accuracy_score`\n",
    "* `MinMaxScaler`\n",
    "* `LabelEncoder`\n",
    "\n",
    "¿Cuál es la precisión (accuracy) del modelo cuando se usa escalado? Compárala con la obtenida con los datos sin transformar (los originales, sin escalar)\n",
    "\n",
    "Muestra un histograma del primer predictor original y otro de la misma variable tras su escalado. ¿Es compatible lo que muestra el histograma con lo que se espera del escalado? Razona la respuesta\n",
    "\n",
    "Repite el mismo ejercicio cambiando el escalado al rango [-1, 1]. Comenta los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5995c09",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# evaluate knn on the diabetes dataset with minmax scaler transform\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(datadir + 'pima-indians-diabetes.csv', header=None)\n",
    "data = dataset.values\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y.astype('str'))\n",
    "# split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "    # define the scaler\n",
    "scalers = [\n",
    "    (\"No Scaling\", IdentityScaler()),\n",
    "    (\"Scale [0, 1]\", MinMaxScaler(feature_range=(0, 1))),\n",
    "    (\"Scale [-1, 1]\", MinMaxScaler(feature_range=(-1, 1))),\n",
    "]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "results = defaultdict(list)\n",
    "\n",
    "for name, scaler in scalers:\n",
    "\n",
    "    # scale train and test sets\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled =scaler.transform(X_test)\n",
    "\n",
    "    # test model with different number of neighbors\n",
    "    print(f\"--- {name} ---\")\n",
    "    for n_neighbors in range(2, 20):\n",
    "        # create model\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "\n",
    "        # train it\n",
    "\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # predict on test set\n",
    "\n",
    "        yhat = knn.predict(X_test_scaled)\n",
    "\n",
    "        # evaluate predictions\n",
    "\n",
    "        accuracy = accuracy_score(yhat, y_test)\n",
    "\n",
    "        results[name].append((n_neighbors, accuracy))\n",
    "\n",
    "        # print accuracy result\n",
    "        print(f'n_neighbors: {n_neighbors:>2}\\tAccuracy: {accuracy:.2%}')\n",
    "\n",
    "# plot histograms\n",
    "for name, data in results.items():\n",
    "    # get just the accuracies\n",
    "    data = np.array(data).T\n",
    "    # plot the histogram\n",
    "    plt.plot(data[0], data[1], \"o-\", label=name)\n",
    "plt.legend()\n",
    "plt.xticks(range(2, 20))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a76cf989",
   "metadata": {},
   "source": [
    "Aunque sea dificil ver en la gráfica, pero ambos escalados producen exactamente los mismos resultados. En cuanto a la comparación con no escalarlos,\n",
    "aunque ambas gráficas no sean monotónicas, se ve que el máximo de la precisión se da en el caso de usar el escalado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fced70",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_feature = X_train[:, 0]\n",
    "\n",
    "first_feature_scaled = MinMaxScaler(feature_range=(0, 1)).fit_transform(first_feature.reshape(-1, 1))\n",
    "\n",
    "plt.hist(first_feature, bins=20, alpha=0.5, label=\"Original\")\n",
    "plt.show()\n",
    "plt.hist(first_feature_scaled, bins=20, alpha=0.5, label=\"Scaled\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49baf56a",
   "metadata": {},
   "source": [
    "Como se ve, ambos histogramas tienen una forma idéntica, lo unico que cambia  son los valores de los ejes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ce5e9",
   "metadata": {},
   "source": [
    "### Estandarización (z-score)\n",
    "\n",
    "[Documentación en scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)\n",
    "\n",
    "¿Cuál es la precisión (accuracy) cuando se usa escalado? Compárala con la obtenida con los datos sin transformar (los originales, sin escalar)\n",
    "\n",
    "Calcula la media y la desviación típica de los predictores originales y compáralos con los de los estandarizados. ¿Son los valores de estos últimos los que esperabas ver? ¿Porqué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d64750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the diabetes dataset with minmax scaler transform\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(datadir + 'pima-indians-diabetes.csv', header=None)\n",
    "data = dataset.values\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y.astype('str'))\n",
    "# split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# define the scaler\n",
    "scalers = [\n",
    "    (\"No Scaling\", IdentityScaler()),\n",
    "    (\"Standardize\", StandardScaler()),\n",
    "]\n",
    "\n",
    "# save the accuracy for plotting\n",
    "from collections import defaultdict\n",
    "results = defaultdict(list)\n",
    "\n",
    "# scale train and test sets\n",
    "for name, scaler in scalers:\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    print(f\"--- {name} ---\")\n",
    "    # test model with different number of neighbors\n",
    "    for n_neighbors in range(2, 20):\n",
    "        # create model\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        # train it\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # predict on test set\n",
    "        yhat = knn.predict(X_test_scaled)\n",
    "\n",
    "        # evaluate predictions\n",
    "        accuracy = accuracy_score(yhat, y_test)\n",
    "\n",
    "        # print accuracy result\n",
    "        print(f'n_neighbors: {n_neighbors:>2}\\tAccuracy: {accuracy:.2%}')\n",
    "        results[name].append((n_neighbors, accuracy))\n",
    "\n",
    "# plot accuracy\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "for name, data in results.items():\n",
    "    data = np.array(data).T\n",
    "    plt.plot(data[0], data[1], \"o-\", label=name)\n",
    "    plt.xticks(data[0])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# mean and standard deviation of predictors before\n",
    "# and after scaling\n",
    "unscaled_means = X.mean(axis=0)\n",
    "unscaled_stds = X.std(axis=0)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)\n",
    "\n",
    "scaled_train_means = X_train_scaled.mean(axis=0)\n",
    "scaled_train_stds = X_train_scaled.std(axis=0)\n",
    "scaled_test_means = X_test_scaled.mean(axis=0)\n",
    "scaled_test_stds = X_test_scaled.std(axis=0)\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(12, 20))\n",
    "\n",
    "def paint_mean_std(ax, means, stds, title):\n",
    "    ax[0].set_title(f\"{title} Means\")\n",
    "    ax[1].set_title(f\"{title} Stds\")\n",
    "    ax[0].bar(range(len(means)), means)\n",
    "    ax[1].bar(range(len(stds)), stds)\n",
    "    ax[0].bar_label(ax[0].containers[0], fmt='%.2f')\n",
    "    ax[1].bar_label(ax[1].containers[0], fmt='%.2f')\n",
    "\n",
    "fig.suptitle(\"Mean and Standard Deviation of Predictors Before and After Scaling\")\n",
    "paint_mean_std(axs[0], unscaled_means, unscaled_stds, \"Unscaled\")\n",
    "paint_mean_std(axs[1], scaled_train_means, scaled_train_stds, \"Scaled Train\")\n",
    "paint_mean_std(axs[2], scaled_test_means, scaled_test_stds, \"Scaled Test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eaa767a2",
   "metadata": {},
   "source": [
    "De nuevo se ve que la precisión de ambos varia segun el número de vecinos utilizado, pero aún así el máximo obtenido resulta del uso de la estandarización.\n",
    "\n",
    "En cuanto a los gráficos de la comparación de la media y desviación típica, se ve que mientras que tienen valores disparados en los datos originales, tras la estandarización la media y la desviación típica son perfectamente 0 y 1 en el conjunto de train y muy cercanos a estos en el test."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "198.25px",
    "width": "298px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "124093b7be10c32076c75f60f415d6def65edeb693f6c465ae4e1e508e9d8137"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
