{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desgarga de datos\n",
    "\n",
    "Los datos utilizados en este estuido es bastante elaborado.\n",
    "Esto se debe a que la fuente de datos solo proporciona \"snapshots\" mensuales de los datos,\n",
    "por lo que en primer lugar habrá que descargarse los datos de cada mes.\n",
    "Además, la cantidad de datos es bastante gigantesca,\n",
    "por lo que se hará preprocesamiento y aggregación de datos\n",
    "de antemano para reducir el tamaño de los datos a la hora de utilizarlos en R.\n",
    "\n",
    "Las únicas librerias necesarias para este notebook son `aiohttp`, `pandas` y `numpy`.\n",
    "\n",
    "Si solo se quiere obtener todos los datos necesarios, basta con ejecutar todas las celdas de este notebook en orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "\n",
    "%autoawait asyncio\n",
    "\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile as zf\n",
    "\n",
    "data_root = './data/'\n",
    "os.makedirs(data_root, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## descarga de la fuente\n",
    "\n",
    "La fuente de datos es http://ratings.fide.com/download_lists.phtml. Los datos son sobre todos los jugadores de ajedrez que estan de alta en la FIDE (Federación Internacional de Ajedrez).\n",
    "La FIDE es la organización que se encarga de organizar los torneos de ajedrez a nivel mundial, y es la que otorga los títulos de ajedrez (GM, IM, FM, etc).\n",
    "\n",
    "Es notable que dado que hay 11 años de \"snapshots\", habra que descargarse y descomprimir aproximadamente 130 archivos. Esto en total son 2GB de datos.\n",
    "\n",
    "Dado que cada descarga y descompensión puede tardar varios segundos, se hará uso de la programación asíncrona para acelerar el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache de archivos descargados\n",
    "months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "\n",
    "def time_range(start: tuple[int, str], end: tuple[int, str]):\n",
    "\tstart_year, start_month = start\n",
    "\tend_year, end_month = end\n",
    "\tstart_idx = months.index(start_month)\n",
    "\tend_idx = months.index(end_month)\n",
    "\tif start_year == end_year:\n",
    "\t\tfor m in months[start_idx:end_idx + 1]:\n",
    "\t\t\tyield (start_year, m)\n",
    "\t\treturn\n",
    "\tfor m in months[start_idx:]:\n",
    "\t\tyield (start_year, m)\n",
    "\tfor y in range(start_year + 1, end_year):\n",
    "\t\tfor m in months:\n",
    "\t\t\tyield (y, m)\n",
    "\tfor m in months[:end_idx + 1]:\n",
    "\t\tyield (end_year, m)\n",
    "\n",
    "study_domain = set(time_range(start=(12, 'aug'), end=(23, 'jun')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_regex = re.compile(r'^(?P<month>\\w{3})_(?P<year>\\d{2})\\.csv$')\n",
    "\n",
    "all_files = os.listdir(data_root)\n",
    "\n",
    "raw_csv_files = [\n",
    "\t(m.group(0), (int(m.group('year')), m.group('month')))\n",
    "\tfor m in map(raw_csv_regex.match, all_files)\n",
    "\tif m\n",
    "]\n",
    "\n",
    "len(raw_csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_years = {y for _, y in raw_csv_files}\n",
    "missing_years = study_domain - existing_years\n",
    "\n",
    "print(f'Faltan {len(missing_years)} archivos por descargar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_names = ['fideid', 'name', 'country', 'sex', 'title', 'w_title', 'o_title', 'rating', 'games', 'k', 'birthday', 'flag']\n",
    "\n",
    "# Los archivos zip pueden ser bastante grandes, por lo que su descompresión puede tomar un tiempo considerable.\n",
    "# Para aliviar esto, se realiza la descompresión en un proceso separado, para que sea en paralelo saltandose el GIL.\n",
    "def read_zip(bin: bytes, month, year) -> pd.DataFrame:\n",
    "    filelike = io.BytesIO(bin)\n",
    "    with zf.ZipFile(filelike) as zip_file:\n",
    "        with zip_file.open(zip_file.namelist()[0]) as xml_file:\n",
    "            print(f\"Starting for {month}_{year}\")\n",
    "            df = pd.read_xml(xml_file, compression=\"zip\")\n",
    "            print(f\"Finished for {month}_{year}\")\n",
    "            df.to_csv(os.path.join(data_root, f'{month}_{year}.csv'), index=False, columns=attribute_names)\n",
    "\n",
    "# Se quiere hacer cada descarga en paralelo, por lo que se define un procedimiento asincrono que descarga un archivo, y entonces se llama a este procedimiento en paralelo.\n",
    "async def download(session: aiohttp.ClientSession, executor: ProcessPoolExecutor, month, year):\n",
    "    async with session.get(f'http://ratings.fide.com/download/standard_{month}{year}frl_xml.zip') as response:\n",
    "        if response.status != 200:\n",
    "            print(f'Error downloading {month}{year} (status: {response.status})')\n",
    "            return\n",
    "\n",
    "        zip_bin: bytes = await response.read()\n",
    "        event_loop = asyncio.get_running_loop()\n",
    "        await event_loop.run_in_executor(executor, read_zip, zip_bin, month, year)\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=5) as executor:\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        all_futs = [download(session, executor, month, year) for year, month in missing_years]\n",
    "        print(f'Downloading {len(all_futs)} files')\n",
    "        grouped = [\n",
    "            all_futs[i:i + 5]\n",
    "            for i in range(0, len(all_futs), 5)\n",
    "        ]\n",
    "        for g in grouped:\n",
    "            await asyncio.gather(*g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregación de datos\n",
    "\n",
    "Ya que 2 GB de datos no son fáciles de manejar, se hará una agregación de datos para reducir el tamaño de los datos.\n",
    "\n",
    "La aggregación principal que se realizará es agrupar por país y año y mes, en vez de por jugador. Esto debería reducir el tamaño de los datos por varias ordenes de magnitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(data_root)\n",
    "raw_csv_files = [\n",
    "\t(m.group(0), (int(m.group('year')), m.group('month')))\n",
    "\tfor m in map(raw_csv_regex.match, files)\n",
    "\tif m\n",
    "]\n",
    "\n",
    "len(raw_csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime as dt\n",
    "\n",
    "def to_datetime(p):\n",
    "\treturn dt(2000 + p[0], months.index(p[1]) + 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import comb\n",
    "agg_dict = defaultdict(list)\n",
    "\n",
    "l = list()\n",
    "\n",
    "for file, period in raw_csv_files:\n",
    "\tfile_path = os.path.join(data_root, file)\n",
    "\tdate = to_datetime(period)\n",
    "\tdf = pd.read_csv(file_path)\n",
    "\tdf = df.loc[lambda d: d['rating'] > 0, :]\n",
    "\tdf = df.loc[lambda d: d['flag'].isna() | ~d['flag'].str.contains('i').replace(np.nan, False), :]\n",
    "\n",
    "\ttotal_mean = df['rating'].mean()\n",
    "\tlambda_reciprocal = total_mean / 400 * np.log(10)\n",
    "\n",
    "\tfor country, group in df.groupby('country'):\n",
    "\n",
    "\n",
    "\t\tagg_dict['country'].append(country)\n",
    "\t\tagg_dict['date'].append(date)\n",
    "\n",
    "\t\t# Agregados de número de jugadores\n",
    "\t\tagg_dict['count'].append(len(group))\n",
    "\n",
    "\t\t# Agregados de titulo\n",
    "\t\ttitle_gm = sum(group['title'] == 'GM')\n",
    "\t\ttitle_wgm = sum(group['title'] == 'WGM')\n",
    "\t\ttitle_im = sum(group['title'] == 'IM')\n",
    "\t\ttitle_wim = sum(group['title'] == 'WIM')\n",
    "\t\ttitle_fm = sum(group['title'] == 'FM')\n",
    "\t\ttitle_wfm = sum(group['title'] == 'WFM')\n",
    "\t\ttitle_cm = sum(group['title'] == 'CM')\n",
    "\t\ttitle_wcm = sum(group['title'] == 'WCM')\n",
    "\n",
    "\t\ttotal_title = title_gm + title_wgm + title_im + title_wim + title_fm + title_wfm + title_cm + title_wcm\n",
    "\n",
    "\t\tagg_dict['title_total'].append(total_title)\n",
    "\n",
    "\n",
    "\t\t# Agregados de rating\n",
    "\t\tagg_dict['mean'].append(group['rating'].mean())\n",
    "\n",
    "\t\tratings = group['rating']\n",
    "\t\ttop_100_mean = ratings.sort_values(ascending=False).head(100).mean()\n",
    "\t\tagg_dict['top_100_mean'].append(top_100_mean)\n",
    "\n",
    "\t\tk = len(group)\n",
    "\t\texpected_best = lambda_reciprocal * sum([1/i for i in range(1, k+1)])\n",
    "\t\tnormalized_top_100 = top_100_mean - expected_best\n",
    "\t\tagg_dict['normalized_top_100'].append(normalized_top_100)\n",
    "\t\tl.append({\n",
    "\t\t\t'country': country,\n",
    "\t\t\t'date': int(date.timestamp()),\n",
    "\t\t\t'count': len(group),\n",
    "\t\t\t'title_total': total_title,\n",
    "\t\t\t'mean': group['rating'].mean(),\n",
    "\t\t\t'top_100_mean': top_100_mean,\n",
    "\t\t\t'normalized_top_100': normalized_top_100,\n",
    "\t\t})\n",
    "\n",
    "agg_df = pd.DataFrame(agg_dict)\n",
    "\n",
    "agg_df.to_csv(os.path.join(data_root, 'agg.csv'), index=False)\n",
    "\n",
    "with open(\"agg.json\", \"w\") as f:\n",
    "\tjson.dump(l, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OECD\n",
    "\n",
    "Los datos vienen de https://stats.oecd.org/index.aspx?DataSetCode=HSL, son descargados y este script sirve para convertirlos a un formato más manejable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"well_being.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(list)\n",
    "\n",
    "indicators = [\n",
    "\t'social_support',\n",
    "\t'feeling_safe_at_night',\n",
    "\t'employment_rate',\n",
    "\t'life_expectancy_at_birth',\n",
    "\t'earnings',\n",
    "\t'housing_affordability'\n",
    "]\n",
    "\n",
    "g = df.groupby(['LOCATION', 'TIME', 'Indicator'])\n",
    "for (country, year, indicator), group in g:\n",
    "\tindicator_normalized = indicator.replace(' ', '_').lower()\n",
    "\tif indicator_normalized not in indicators:\n",
    "\t\tcontinue\n",
    "\td[\"country\"].append(country)\n",
    "\td[\"year\"].append(year)\n",
    "\td[\"variable\"].append(indicator)\n",
    "\td[\"value\"].append(group['Value'].mean())\n",
    "\n",
    "df_wb = pd.DataFrame(d).reset_index(drop=True)\n",
    "\n",
    "df_wb.to_csv(os.path.join(data_root, 'well_being.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "l = []\n",
    "for (country, year, indicator), group in g:\n",
    "\tl.append({\n",
    "\t\t'country': country,\n",
    "\t\t'year': int(year),\n",
    "\t\t'variable': indicator,\n",
    "\t\t'value': float(group['Value'].mean())\n",
    "\t})\n",
    "\n",
    "with open(\"well_being.json\", 'w') as f:\n",
    "\tjson.dump(l, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
